% Gaussian Process

clear
clc
close all

% Gaussian Process (GP) are kernel methods originally applied to regression
% problemes. They assume the problem that has an underlying Gaussian
% distribution by extending the concept of multivariate Gaussian
% distribution to the infinite dimensional case. They assume that there
% exists a Gaussian distribution over each target t given the input sample
% x and there exists a covariance between the expected values of two
% targets t_i and t_j of the form:
% 
% E[y(x_i)y(x_j)] = K(x_i, x_j) = K_ij
%
% We instantiate a GP with the function qprfit. For instance if we want to
% instantiate a GP for the regression of the sepal length t by using the
% sepal width x with a kernel and a Gaussian noise.

load iris_dataset.mat

gplotmatrix(irisInputs');
x = zscore(irisInputs(3,:)');
t = zscore(irisInputs(4,:)');

GPmodel = fitrgp(x,t, 'KernelFunction', 'squaredexponential');

% While the process of learning a GP does not require any computation other
% than the gram matrix K, the parameters of the kernel should be tuned
% properly to get meaningful results. This is why MATLAB requires a
% training phase. 

theta = GPmodel.KernelInformation.KernelParameters;
sigma = GPmodel.Sigma;

% Where the first are the kernel parameters and the last one is the
% estimation of the noise standard deviation.

% Let us look how the parameters and the noise influence the shape of the
% GP.

theta1 = [theta(1) 10 * theta(2)];
GPmodel = fitrgp(x,t, 'KernelParameters', theta1, 'Sigma', sigma, 'FitMethod', 'none');
plotGp(GPmodel);

theta2 = [theta(1) 0.1 * theta(2)];
GPmodel = fitrgp(x,t, 'KernelParameters', theta2, 'Sigma', sigma, 'FitMethod', 'none');
plotGp(GPmodel);

% The higher the parameter the more the data is influencing the expected
% values of the final GP. If theta_1 is small the resulting GP predicts a more
% average prediction, where the average model is generated by the noise we
% considered. 

% If we change the bandwidth of the kernel theta_1, we see the prediction
% depends more or less from the points which are near to the prediction
% one. This influence the smoothness of the GP average function. Finally,
% if we change noise standard deviation at each point of the input space we
% have distributions which have different variance. 

% User defined kernel function
hand = @gaussKer;

GPmodel = fitrgp(x,t, 'KernelFunction', hand, 'KernelParameters', [1 1], 'Sigma', 1);
plotGp(GPmodel);

exp(GPmodel.KernelInformation.KernelParameters)

% The result is equivalent (up to numerical errors) to what has been
% obtained with the default GP optimization.

% Finally we can implement the GP (with given parameters and noise
% variance) in the following way:

% GP by hand

theta0 = log(theta);
sigma0 = sigma;
n_train = 150;
n_new = 100;

x_new = linspace(min(x),max(x),n_new)';

K = gaussKer(x, x, theta0) + sigma0 * eye(n_train);
for ii = 1:n_new
    Kstar = gaussKer(x_new(ii), x, theta0);
    Kstarstar = gaussKer(x_new(ii), x_new(ii), theta0) + sigma0^2;

    y_new(ii) = Kstar * pinv(K) * t;
    y_var(ii) = Kstarstar - Kstar * pinv(K) * Kstar';

end

y_sup = y_new + 2 * sqrt(y_var);
y_inf = y_new - 2 * sqrt(y_var);
figure()
hold on;
plot(x, t, 'r.');
plot(x_new, y_new, 'b', 'LineWidth', 2);
h1 = fill([x_new' x_new(end:-1:1)'],[y_sup y_inf(end:-1:1)],'k');
alpha(h1,.2);
axis tight;


